\documentclass[12pt]{article}
%\pointpoints{punto}{puntos}

\usepackage{amsfonts, geometry, amsmath, amssymb, hyperref, cite, amsthm}
\usepackage{algpseudocode, algorithm}
\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage{tikzit}
\input{sample.tikzstyles}

\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{exmp}{Example}[section]

\theoremstyle{remark}
\newtheorem*{rem}{Remark}

\def\NN{\ensuremath{\mathbb{N}}}
\def\N{\ensuremath{\mathbb{N}}}
\def\ZZ{\ensuremath{\mathbb{Z}}}
\def\Z{\ensuremath{\mathbb{Z}}}
\def\QQ{\ensuremath{\mathbb{Q}}}
\def\Q{\ensuremath{\mathbb{Q}}}
\def\RR{\ensuremath{\mathbb{R}}}
\def\R{\ensuremath{\mathbb{R}}}
\def\CC{\ensuremath{\mathbb{C}}}
\def\FF{\ensuremath{\mathbb{F}}}
\def\Pr{\ensuremath{\mbox{Pr}}}


\usepackage{lastpage}
\usepackage[none]{hyphenat}


%%--MATH---------------------------------------------------------

%%--OTHER ENVIRONMENTS--------------------------------------
\renewcommand{\labelenumi}{{\bf \theenumi.}}
\renewcommand{\labelenumii}{{\bf (\theenumii)}}
\renewcommand{\labelenumiii}{{\bf (\theenumiii)}}
%%----------------------------------------------------------
\begin{document}
\title{Graduation Project}
\author{Santiago Morales - 201913369}
\date{\today}

\maketitle
\section{Introduction}

\section{Kahn-Kalai Conjecture}

\subsection{Thresholds}

Let $n \in \N$ and $0 \leq p \leq 1$. The random graph $G(n, p)$ is a probability space over the set of graphs on $n$ labeled vertices determined by
\[\Pr[\{i, j\} \in G] = p\] 
with these events mutually independent \cite{alon2016probabilistic}. Given a graph theoretic property $A$, there is a probability that $G(n, p)$ satisfies $A$, which we write as $\Pr[G(n, p) \vDash A]$. 

\begin{definition}
    $r(n)$ is a threshold function for a graph theoretic property $A$ if 
    \begin{enumerate}
        \item When \(p(n) \in o(r(n)), \; \lim_{n \to \infty} \Pr[G(n, p(n)) \vDash A] = 0,\)
        \item When \(r(n) \in o(p(n)), \;  \lim_{n \to \infty} \Pr[G(n, p(n)) \vDash A] = 1,\) 
    \end{enumerate}
    or vice versa. \cite{alon2016probabilistic}
\end{definition}

We give an example of a threshold function which illustrates a common method for proving that a function is a threshold. \par

\subsubsection{Threshold function for having isolated vertices}

Let $G$ be a graph on $n$ labeled vertices. An isolated vertex of $G$ is a vertex which does not belong to any of the edges of $G$. Let $A$ be the property that $G$ contains an isolated vertex. We will prove that $\displaystyle{r(n) = \frac{\ln n}{n}}$ is a threshold for $A$. \par

For each vertex $i$ in $G$ define the variable 

\[X_i = 
\left\{
	\begin{array}{ll}
		1  & \mbox{if } i \text{ is an isolated vertex,} \\
		0 & \mbox{if } i \text{ is not an isolated vertex.}
	\end{array}
\right.
\]

Now, the probability that a vertex $i$ is isolated is $(1 - p)^{n - 1}$ since it is the probability that none of the other $n - 1$ vertices is connected to $i$. Let $X = \sum_{i = 1}^n X_i$, then the expected number of isolated vertices is
 \[E[X] = \sum_{i = 1}^{n} E[X_i] = \sum_{i = 1}^{n} \Pr[X_i] = n(1 - p)^{n - 1}.\]

Let $\displaystyle{p = k\frac{\ln n}{n}}$ for $k \in \R_{>0}$. Then
\begin{align*}
    \lim_{n \to \infty} E[X] &= \lim_{n \to \infty} n\left(1 - k\frac{\ln n}{n}\right)^{n - 1} \\
    &= ne^{-k\ln n} = n^{1 - k}.
\end{align*}

Therefore, $\lim_{n \to \infty} E[X] = 0$ if $k > 1$. Since \(E[X] \geq \Pr[X > 0],\) we conclude that \[\lim_{n \to \infty} \Pr[G(n, p) \vDash A] =  \lim_{n \to \infty} \Pr[X > 0] = 0.\] \par
Now, for $k < 1$, the fact that $\lim_{n \to \infty} E[X] = \infty$ is not enough to conclude that \(\lim_{n \to \infty} \Pr[G(n, p) \vDash A] = 1\). We have to use the second moment method. \par

\begin{theorem*}
    If $E[X] \to \infty$ and $\text{Var}[X] = o(E[X]^2)$, then $\lim_{n \to \infty} \Pr[X > 0] = 1$. \cite{alon2016probabilistic}
\end{theorem*}

\textbf{Proof. } We will prove that, in this case, $Var[X] = o(E[X]^2)$. First, 
\begin{align*}
    \sum_{i \neq j}E[X_iX_j] &= \sum_{i \neq j} \Pr[X_i = X_j = 1] \\
    &= n(n - 1)(1 - p)^{n -1}(1 - p)^{n - 2} \\ &= n(n - 1)(1 - p)^{2n - 3},
\end{align*}

for if $i$ is an isolated vertex, then there is no edge between $i$ and $j$ so we only have to account for the remaining $n - 2$ edges that contain $j$.  \par

Thus, since $\sum_{i = 1}^{n}E[X_i^2] =  \sum_{i = 1}^n E[X_i] = E[X]$ and $\lim_{n \to \infty} p = 0$,

\begin{align*}
    \lim_{n \to \infty} \frac{\text{Var}[X]}{E[X]^2} &= 
    \lim_{n \to \infty}\frac{E(X^2) - E[X]^2}{E[X]^2} = \lim_{n \to \infty} \frac{\sum_{i = 1}^n E[X_i^2] + \sum_{i \neq j}E[X_iX_j]}{E[X]^2} - 1 \\ &= 0 + \lim_{n \to \infty} \frac{ n(n - 1)(1 - p)^{2n - 3}}{n^2(1 - p)^{2n - 2}} - 1= \lim_{n \to \infty} \frac{1}{1 - p} - 1 = 0.\\
\end{align*} \par
We conclude that $\text{Var}[X] \in o(E[X]^2)$ and so, if $k < 1$, \[\lim_{n \to \infty} \Pr[G(n, p) \vDash A] = \lim_{n \to \infty} \Pr[X > 0] = 1.\] Therefore, $r(n) = \frac{\ln n}{n}$ is a threshold function for property $A$.

\subsection{The expectation threshold}

Let $X$ be a set and $p \in [0, 1]$. For $A \subset X$, let $P(A) = p^{|A|}(1 - p)^{|X \setminus A|}$. A class $\mathcal{F}$ of subsets of $X$ is increasing if for $A \in \mathcal{F}$, $A \subset B$ implies $B \in \mathcal{F}$. If $\mathcal{F} \neq \emptyset$,  $P(\mathcal{F}) = \sum_{A \in \mathcal{F}} P(A)$ is a strictly increasing function of $p$.
 
\textbf{TODO} Define increasing class and general definition of threshold \cite{park2022proof} 

\noindent \textbf{TODO} Define expectation threshold and show inequalities. \cite{frankston2021thresholds}

\begin{exmp}
    $H_1$ example
    \begin{figure}[h]
        \centering
        \ctikzfig{H1}
        \caption{$H_1$}
        \label{H1}
    \end{figure}
\end{exmp}

\begin{exmp}
    $H_2$ example
    \begin{figure}[h]
        \centering
        \ctikzfig{H2}
        \caption{$H_2$}
        \label{H2}
    \end{figure}
    
\end{exmp}

\begin{theorem}[Park Theorem \cite{park2022proof}, originally Kahn-Kalai Conjecture]
\textbf{TODO}
\end{theorem}

\textbf{TODO} How to prove something is p-small

\subsubsection{An application of the Park Theorem}

\noindent \textbf{TODO} Prove the threshold for perfect matchings for $G(n, p)$

\section{Numerical Semigroups}

A \textit{numerical semigroup} is a subset $S \subset \NN_{0}$ which is closed under addition, i.e. $a, b \in S$ implies $a + b \in S$. For instance, $\NN_0$, $\NN_0 \setminus \{0\}$, $2\NN_0$ are all numerical semigroups, but $\NN_0 \setminus \{2\}$ is not. Some literature requires that a semigroup has a finite complement in $\Z_{\geq 0}$ \cite{chapman2020beyond}, but we prefer the more general definition. 

\begin{exmp} \textbf{TODO}
    McNugget Semigroup
\end{exmp}

As seen in the previous example, we can describe a numerical semigroup with a set of \textit{generators}: 
\[S = \langle a_1, ..., a_n \rangle := \left\{\sum_{i = 1}^n k_ia_i: k_1,...,k_n \in \NN_0\right\}.\] \textbf{TODO} give some examples.

The \textit{Frobenius number} $F(S)$ of a numerical semigroup $S = \langle a_1, ..., a_n \rangle$ is defined as the largest integer divisible by $d(S) := \gcd(a_1, ..., a_n)$ which does not belong to $S$. In other terms, 
\[F(S) = \max (d \ZZ \setminus S).\]

\section{Probability Spaces over Numerical Semigroups}

We generate a random numerical semigroup with a model similar to the Ã‹rdos-Renyi model for random graphs. 

\begin{definition}
    For $p \in [0, 1]$ and $M \in \NN$, a random numerical semigroup $S(M, p)$ is a probability space over the set of semigroups $S = \langle\mathcal{A}\rangle$ with $\mathcal{A} \subset \{1,...,M\}$, determined by
    \[\text{Pr}[n \in \mathcal{A}] = p,\]
    with these events mutually independent.
\end{definition}


\section{Expected Frobenius Number}

We prove a Theorem found in \cite{de2017random} without the use of the simplicial complex. 

\begin{theorem}
    Let $S \sim S(M, p)$, where $p = p(M)$ is a monotone decreasing function of M. If $\frac{1}{M} \ll p \ll 1$, then $S$ is cofinite, i.e., the set of gaps is finite, a.a.s and 
\[\lim_{M \to \infty} E[e(S)] = \lim_{M \to \infty} E[g(S)] = \lim_{M \to \infty} E[F(S)] = \infty.\]
\end{theorem}

\textbf{Proof. } Let $X := \min(S \setminus \{0\})$ be a random variable. Then, for $0 < n \leq M$,
\begin{align*}
    P[X = n] = p(1 - p)^{n - 1}, 
\end{align*}
and so 
\begin{align*}
    E[X] &= \sum_{n = 0}^{\infty} nP[X = n] =
    \sum_{n = 0}^{M} np(1 - p)^{n - 1} = p\frac{d}{dp}\left[-\sum_{n = 0}^{M}(1 - p)^n\right]\\
    &=p\frac{d}{dp} \frac{(1 - p)^{M + 1} - 1}{p} = p\frac{1 - (1 - p)^{M + 1} - (M  + 1)(1 - p)^Mp}{p^2} \\
    &= \frac{1 - (1 - p)^{M} - M(1 - p)^Mp}{p} \geq \frac{1 - e^{-Mp} - Mpe^{-Mp}}{p}. \\
\end{align*}
Thus, since $\lim_{M \to \infty} Mp = \infty$, then $\lim_{M \to \infty }Mpe^{-Mp} = \lim_{M \to \infty} e^{-Mp} = 0$, which means
\[\lim _{M \to \infty} E[X] = \lim_{M \to \infty} \frac{1 - e^{-Mp} - Mpe^{-Mp}}{p} = \infty.\]
Also, note that if $p = \frac{c}{M}$, $c \in \mathbb{R_+}$ ($0 < e^{-c} + ce^{-c} < 1$),
\[\lim _{M \to \infty} E[X] = \lim_{M \to \infty} \frac{1 - e^{-c} - ce^{-c}}{p} = \infty.\]

\bibliography{refs}
\bibliographystyle{ieeetr}

\end{document}
