% All contribution chapters should follow a similar structure, with a
% mini-introduction and overview at the beginning and a conclusion at the
% end bookmarking a structured presentation of the contribution. This can be
% largely based on your publications.

\chapter{Contribution 1}\label{chap:contrib1}

\section{Introduction}

In this Chapter, XXX is presented. Note: to cross-reference to other parts of the document you do so like this - see Section \ref{sec:contrib2:theme1:B}.

Section \ref{sec:contrib1:theme1} discusses Theme 1. Section \ref{sec:contrib1:theme2} discusses Theme 2....

\section{Theme 1}\label{sec:contrib1:theme1}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{sample_wide_figure}
\caption{This the approximate process for producing a Thesis. It typically takes 3-4 years.}
\label{fig:phd_life}
\end{figure}

\figurename{} \ref{fig:phd_life} illustrates the key elements of Thesis synthesis. \LaTeX{} will tend to place figures where it wants (they `float' - generally they should be at the top or bottom of a page); you can override the default behaviour if you want, but you probably don't want to bother doing that until after your content is pretty much done. Instead, keep the figures as close as possible to the text; you can tweak this afterwards if you want by adding an option:

\begin{verbatim}
\begin{figure}[htb]
\end{verbatim}

This says put it here, if you can, otherwise at the top, or otherwise at the bottom. BUT I strongly suggest not using {\bf h} as it looks terrible.

Maybe you need an inline URL at this point: Here's one! \url{https://en.wikibooks.org/wiki/LaTeX}.

Mathematics can be inline, for example $x = \int_0^\infty y^2 dy$, or can be in display mode, as shown in \eqref{eqn:example}:

\begin{equation}
\label{eqn:example}
F(x,y,z) = \sqrt{x^{y^z}}
\end{equation}

You probably want to show some of your results in a table, such as \tablename{} \ref{tab:example}.

% Table environment. This is the most difficult thing to get right in LaTeX if you go beyond a simple table like that shown below. Refer to the LaTeX Wikibook for lots of info about creating LaTeX tables (see the toplevel README.md).
\begin{table*}
\centering
\caption{Table captions normally go at the top.}
\label{tab:example}
% Don't leave blank lines in the middle of a floating environment such as table or figure!
% This bit describes how the columns are organised. l, r, c for left, right, centre-justified; p{1cm} or p{0.2\textwidth} for left-justified with fixed width. This is usually best.
\begin{tabular}{p{0.2\textwidth}p{0.3\textwidth}p{0.3\textwidth}}
\toprule
{\bf Left column}	&{\bf $\mu$}	&{\bf $\sigma$}\\
\midrule
Item 1				&0.3			&0.5\\
Item 2				&0.9			&0.4\\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}
\centering
\subfloat[Confusion]{
	\includegraphics[height=0.3\textwidth]{confusion}
	\label{fig:confusion}
}
\subfloat[Work]{
	\includegraphics[height=0.3\textwidth]{work}
	\label{fig:work}
}
\subfloat[Exhaustion]{
        \includegraphics[height=0.3\textwidth]{exhaustion}
        \label{fig:exhaustion}
}
\caption{The stages of the creative process.}
\label{fig:creative_process}
\end{figure}

The basic creative process is shown in \figurename{} \ref{fig:creative_process}. Specifically, \figurename{} \ref{fig:confusion} shows the first step, while \figurename{} \ref{fig:work} and \ref{fig:exhaustion} show the remaining key steps of the procedure.

\subsection{Subtopic A}\label{sec:contrib1:theme1:A}

Algorithm \ref{alg:cap} shows a classical algorithm typeset in \LaTeX{}. This is also a float.

\begin{algorithm}
\caption{An algorithm with caption}\label{alg:cap}
\begin{algorithmic}
\Require $n \geq 0$
\Ensure $y = x^n$
\State $y \gets 1$
\State $X \gets x$
\State $N \gets n$
\While{$N \neq 0$}
\If{$N$ is even}
    \State $X \gets X \times X$
    \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
\ElsIf{$N$ is odd}
    \State $y \gets y \times X$
    \State $N \gets N - 1$
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Subtopic B}\label{sec:contrib1:theme1:B}

\subsection{Subtopic C}\label{sec:contrib1:theme1:C}

\section{Theme 2}\label{sec:contrib1:theme2}

Etc. etc.

\section{Introduction}

\section{Kahn-Kalai Conjecture}

\subsection{Thresholds}

Let $n \in \N$ and $0 \leq p \leq 1$. The random graph $G(n, p)$ is a probability space over the set of graphs on $n$ labeled vertices determined by
\[\Pr[\{i, j\} \in G] = p\] 
with these events mutually independent \cite{alon2016probabilistic}. Given a graph theoretic property $A$, there is a probability that $G(n, p)$ satisfies $A$, which we write as $\Pr[G(n, p) \vDash A]$. 

\begin{definition}
    $r(n)$ is a threshold function for a graph theoretic property $A$ if 
    \begin{enumerate}
        \item When \(p(n) \in o(r(n)), \; \lim_{n \to \infty} \Pr[G(n, p(n)) \vDash A] = 0,\)
        \item When \(r(n) \in o(p(n)), \;  \lim_{n \to \infty} \Pr[G(n, p(n)) \vDash A] = 1,\) 
    \end{enumerate}
    or vice versa. \cite{alon2016probabilistic}
\end{definition}

We give an example of a threshold function which illustrates a common method for proving that a function is a threshold. \par

\subsubsection{Threshold function for having isolated vertices}

Let $G$ be a graph on $n$ labeled vertices. An isolated vertex of $G$ is a vertex which does not belong to any of the edges of $G$. Let $A$ be the property that $G$ contains an isolated vertex. We will prove that $\displaystyle{r(n) = \frac{\ln n}{n}}$ is a threshold for $A$. \par

For each vertex $i$ in $G$ define the variable 

\[X_i = 
\left\{
	\begin{array}{ll}
		1  & \mbox{if } i \text{ is an isolated vertex,} \\
		0 & \mbox{if } i \text{ is not an isolated vertex.}
	\end{array}
\right.
\]

Now, the probability that a vertex $i$ is isolated is $(1 - p)^{n - 1}$ since it is the probability that none of the other $n - 1$ vertices is connected to $i$. Let $X = \sum_{i = 1}^n X_i$, then the expected number of isolated vertices is
 \[E[X] = \sum_{i = 1}^{n} E[X_i] = \sum_{i = 1}^{n} \Pr[X_i] = n(1 - p)^{n - 1}.\]

Let $\displaystyle{p = k\frac{\ln n}{n}}$ for $k \in \R_{>0}$. Then
\begin{align*}
    \lim_{n \to \infty} E[X] &= \lim_{n \to \infty} n\left(1 - k\frac{\ln n}{n}\right)^{n - 1} \\
    &= ne^{-k\ln n} = n^{1 - k}.
\end{align*}

Therefore, $\lim_{n \to \infty} E[X] = 0$ if $k > 1$. Since \(E[X] \geq \Pr[X > 0],\) we conclude that \[\lim_{n \to \infty} \Pr[G(n, p) \vDash A] =  \lim_{n \to \infty} \Pr[X > 0] = 0.\] \par
Now, for $k < 1$, the fact that $\lim_{n \to \infty} E[X] = \infty$ is not enough to conclude that \(\lim_{n \to \infty} \Pr[G(n, p) \vDash A] = 1\). We have to use the second moment method. \par

\begin{theorem*}
    If $E[X] \to \infty$ and $\text{Var}[X] = o(E[X]^2)$, then $\lim_{n \to \infty} \Pr[X > 0] = 1$. \cite{alon2016probabilistic}
\end{theorem*}

\textbf{Proof. } We will prove that, in this case, $Var[X] = o(E[X]^2)$. First, 
\begin{align*}
    \sum_{i \neq j}E[X_iX_j] &= \sum_{i \neq j} \Pr[X_i = X_j = 1] \\
    &= n(n - 1)(1 - p)^{n -1}(1 - p)^{n - 2} \\ &= n(n - 1)(1 - p)^{2n - 3},
\end{align*}

for if $i$ is an isolated vertex, then there is no edge between $i$ and $j$ so we only have to account for the remaining $n - 2$ edges that contain $j$.  \par

Thus, since $\sum_{i = 1}^{n}E[X_i^2] =  \sum_{i = 1}^n E[X_i] = E[X]$ and $\lim_{n \to \infty} p = 0$,

\begin{align*}
    \lim_{n \to \infty} \frac{\text{Var}[X]}{E[X]^2} &= 
    \lim_{n \to \infty}\frac{E(X^2) - E[X]^2}{E[X]^2} = \lim_{n \to \infty} \frac{\sum_{i = 1}^n E[X_i^2] + \sum_{i \neq j}E[X_iX_j]}{E[X]^2} - 1 \\ &= 0 + \lim_{n \to \infty} \frac{ n(n - 1)(1 - p)^{2n - 3}}{n^2(1 - p)^{2n - 2}} - 1= \lim_{n \to \infty} \frac{1}{1 - p} - 1 = 0.\\
\end{align*} \par
We conclude that $\text{Var}[X] \in o(E[X]^2)$ and so, if $k < 1$, \[\lim_{n \to \infty} \Pr[G(n, p) \vDash A] = \lim_{n \to \infty} \Pr[X > 0] = 1.\] Therefore, $r(n) = \frac{\ln n}{n}$ is a threshold function for property $A$.

\subsection{The expectation threshold}

Let $X$ be a set and $p \in [0, 1]$. For $A \subset X$, let $P(A) = p^{|A|}(1 - p)^{|X \setminus A|}$. A class $\mathcal{F}$ of subsets of $X$ is increasing if for $A \in \mathcal{F}$, $A \subset B$ implies $B \in \mathcal{F}$. If $\mathcal{F} \neq \emptyset$,  $P(\mathcal{F}) = \sum_{A \in \mathcal{F}} P(A)$ is a strictly increasing function of $p$.
 
\textbf{TODO} Define increasing class and general definition of threshold \cite{park2022proof} 

\noindent \textbf{TODO} Define expectation threshold and show inequalities. \cite{frankston2021thresholds}

\begin{theorem}[Park Theorem \cite{park2022proof}, originally Kahn-Kalai Conjecture]
\textbf{TODO}
\end{theorem}

\textbf{TODO} How to prove something is p-small

\subsubsection{An application of the Park Theorem}

\noindent \textbf{TODO} Prove the threshold for perfect matchings for $G(n, p)$

\section{Numerical Semigroups}

A \textit{numerical semigroup} is a subset $S \subset \NN_{0}$ which is closed under addition, i.e. $a, b \in S$ implies $a + b \in S$. For instance, $\NN_0$, $\NN_0 \setminus \{0\}$, $2\NN_0$ are all numerical semigroups, but $\NN_0 \setminus \{2\}$ is not. Some literature requires that a semigroup has a finite complement in $\Z_{\geq 0}$ \cite{chapman2020beyond}, but we prefer the more general definition. 

\begin{exmp} \textbf{TODO}
    McNugget Semigroup
\end{exmp}

As seen in the previous example, we can describe a numerical semigroup with a set of \textit{generators}: 
\[S = \langle a_1, ..., a_n \rangle := \left\{\sum_{i = 1}^n k_ia_i: k_1,...,k_n \in \NN_0\right\}.\] \textbf{TODO} give some examples.

The \textit{Frobenius number} $F(S)$ of a numerical semigroup $S = \langle a_1, ..., a_n \rangle$ is defined as the largest integer divisible by $d(S) := \gcd(a_1, ..., a_n)$ which does not belong to $S$. In other terms, 
\[F(S) = \max (d \ZZ \setminus S).\]

\section{Probability Spaces over Numerical Semigroups}

We generate a random numerical semigroup with a model similar to the Ërdos-Renyi model for random graphs. 

\begin{definition}
    For $p \in [0, 1]$ and $M \in \NN$, a random numerical semigroup $S(M, p)$ is a probability space over the set of semigroups $S = \langle\mathcal{A}\rangle$ with $\mathcal{A} \subset \{1,...,M\}$, determined by
    \[\Pr[n \in \mathcal{A}] = p,\]
    with these events mutually independent.
\end{definition}


\section{Expected Frobenius Number}

We prove a Theorem found in \cite{de2017random} without the use of the simplicial complex. 

\begin{theorem}
    Let $S \sim S(M, p)$, where $p = p(M)$ is a monotone decreasing function of M. If $\frac{1}{M} \ll p \ll 1$, then $S$ is cofinite, i.e., the set of gaps is finite, a.a.s and 
\[\lim_{M \to \infty} E[e(S)] = \lim_{M \to \infty} E[g(S)] = \lim_{M \to \infty} E[F(S)] = \infty.\]
\end{theorem}

\textbf{Proof. } Let $X := \min(S \setminus \{0\})$ be a random variable. Then, for $0 < n \leq M$,
\begin{align*}
    \Pr[X = n] = p(1 - p)^{n - 1}, 
\end{align*}
and so 
\begin{align*}
    E[X] &= \sum_{n = 0}^{\infty} n\Pr[X = n] =
    \sum_{n = 0}^{M} np(1 - p)^{n - 1} = p\frac{d}{dp}\left[-\sum_{n = 0}^{M}(1 - p)^n\right]\\
    &=p\frac{d}{dp} \frac{(1 - p)^{M + 1} - 1}{p} = p\frac{1 - (1 - p)^{M + 1} - (M  + 1)(1 - p)^Mp}{p^2} \\
    &= \frac{1 - (1 - p)^{M} - M(1 - p)^Mp}{p} \geq \frac{1 - e^{-Mp} - Mpe^{-Mp}}{p}. \\
\end{align*}
Thus, since $\lim_{M \to \infty} Mp = \infty$, then $\lim_{M \to \infty }Mpe^{-Mp} = \lim_{M \to \infty} e^{-Mp} = 0$, which implies that
\[\lim _{M \to \infty} E[X] = \lim_{M \to \infty} \frac{1 - e^{-Mp} - Mpe^{-Mp}}{p} = \infty.\]
Also, note that if $p = \frac{c}{M}$, $c \in \mathbb{R_+}$ ($0 < e^{-c} + ce^{-c} < 1$),
\[\lim _{M \to \infty} E[X] = \lim_{M \to \infty} \frac{1 - e^{-c} - ce^{-c}}{p} = \infty.\]
\newpage
\textbf{Proof. }
Fix $a \in \NN$ such that $a > 11$ and let $A = \{1, \ldots, \lfloor\frac{a}{p}\rfloor\}$. Since   $\frac{1}{M} \ll p$, we have that $\lfloor\frac{a}{p}\rfloor \leq M$ for large enough $M$. Consider the following events:
\begin{itemize}
    \item $E_1$: No generator selected is less than $\frac{1}{ap}$.  \par
    Let $X_1$ be the number of generators selected from $\{1,\ldots,\lfloor\frac{1}{ap}\rfloor\}$. Then 
    \[\Pr[\overline{E_1}] = \Pr[X_1 > 0] \leq E[X_1] \leq p \cdot \frac{1}{ap} = \frac{1}{a}.\]

    \item $E_2:$ At most $\frac{3a}{2}$ generators are selected from $A$.\par 
    Let $X_2$ be the number of generators selected in $A$, then $X_2$ is a binomial random variable with $n = \frac{a}{p}$ and we can use the bound (Feller \textcolor{blue}{I can add this to the appendix})

    \[\Pr[\overline{E_2}] = \Pr\left[X_2 > \frac{3a}{2}\right] \leq  \frac{\frac{3a}{2}(1 - p)}{(\frac{3a}{2} - a)^2} \leq \frac{6}{a}.\]

    Also, note that by the union bound \[\Pr[E_1\land E_2] \leq 1 - \frac{1}{a} - \frac{6}{a} = 1 - \frac{7}{a}.\]  

    \item $E_3:$ At least $\frac{a}{2}$ generators are selected from $A$. \par 
    Similarly, we can use the bound for the other tail of the distribution so that 
    \[\Pr[ \overline{E_3}] = \Pr\left[X_2 < \frac{a}{2}\right] \leq  \frac{(n - \frac{a}{2})p}{(np - \frac{a}{2})^2} = \frac{a - (\frac{a}{2})p}{(\frac{a}{2})^2} \leq \frac{4}{a}.\]
    
    \item $E_4:$ The generators selected from $A$ are minimal.\par 
    Let $Y_{(1)}, Y_{(2)}, \ldots, Y_{(k)}$ denote the first $k$ generators selected in $A$. Assume $E_1$ and $E_2$. We have that $E_1$ implies $Y_{(1)} \geq \frac{1}{ap}$ and $E_2$ implies $k \leq \frac{3a}{2}$. \par
    First we bound for the probability that, given $E_1$ and $E_2$, $b \in A$ is selected as a generator. By conditional probability 
    \begin{align*}
        \Pr[b \text{ is selected}] &= \Pr[b \text{ is selected}|E_1 \land E_2]\Pr[E_1 \land E_2]  \\
        &+  \Pr[b \text{ is selected}|\overline{E_1 \land E_2}]\Pr[\overline{E_1 \land E_2}],
    \end{align*}
    and so
    \[\Pr[b \text{ is selected}|E_1 \land E_2] \leq \frac{\Pr[b \text{ is selected}]}{\Pr[E_1 \land E_2]} \leq  \frac{p}{1 - \frac{7}{a}}.  \] 
    Now, note that $Y_{(2)}$ is not minimal if a multiple of $Y_{(1)}$ is selected in $A$. Thus, if we fix $Y_{(1)} = y_1 \geq \frac{1}{ap}$, $Y_{(1)}$ is not minimal if $b \in \{2y_1, 3y_1, \ldots, c_1y_1\}$ is selected, where $c_1y_1$ is the largest multiple of $y_1$ which does not exceed $\frac{a}{p}$. Since $y_1 \geq \frac{1}{ap}$, we have that $c_1 \leq a^2$. Then, using the union bound, 
    \[\Pr[Y_{(2)} \text{ is not minimal}|E_1\land E_2 \land Y_{(1)} = y_1] \leq \frac{pa^2}{1 - \frac{7}{a}} .\]
    If we sum over all possible $y_1$, we get that 
    \[\Pr[Y_{(2)} \text{ is not minimal}|E_1\land E_2] \leq \frac{pa^2}{1 - \frac{7}{a}} .\]
    Similarly, for $2 \leq t \leq k$ and fixed $Y_{(1)} = y_1, \ldots, Y_{(t - 1)} = y_{t - 1}$, $Y_{(t)}$ is not minimal if the first $t - 1$ numbers selected from $A$ can generate $Y_{(t)}$. For the possible numbers generated by the first $t$ numbers selected, there are at most $a^2$ choices for each coefficient, so there are at most $a^{2t}$ such linear combinations. Then 
    \[\Pr[Y_{(t)} \text{ is not minimal}|E_1\land E_2] \leq \frac{pa^{2t}}{1 - \frac{7}{a}} .\]
    Therefore, since $Y_{(1)}$ is always minimal, we can use the union bound and $k \leq \frac{3a}{2}$ to conclude that
    \[\Pr[E_4|E_1 \land E_2] \geq 1 - \frac{p}{1 - \frac{7}{a}}\sum_{t = 1}^{\frac{3a}{2} - 1}a^{2t} = 1 - o(1).\]
    Thus,  
    \[\Pr[E_4] = \Pr[E_4| E_1 \land E_2]\Pr[E_1\land E_2] \geq 1 - \frac{7}{a} - o(1).\] 
\end{itemize}


Finally, note that by union bound, 
\[\Pr[E_4 \land E_3] \geq 1 - \frac{11}{a} - o(1).\] 
Therefore, for every $N \in \NN$ and $\varepsilon > 0$, there exists $K$ such that $M \geq K$ implies \[\Pr[f(S) > N], \; \Pr[g(S) > N], \; \Pr[e(S) > N] > 1 - \varepsilon.\].



\section{Conclusion}
