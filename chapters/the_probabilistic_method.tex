\chapter{The Probabilistic Method}\label{chap:probmet}

\section{Introduction}\label{sec:probmet:intro}

The probabilistic method is a powerful tool, with applications in Combinatorics, Graph Theory, Number Theory and Computer Science. It is a nonconstructive method that proves the existence of an object with a certain property, by showing that the probability that a randomly chosen object has that property is greater than zero. The method requires an appropiate sample space and is best illustrated by an example: \par

\begin{definition}\label{def:tournament}
A \textit{tournament} is a directed graph $T$ on $n$ vertices such that for every pair of vertices $i, j \in V(T)$, exactly one of the edges $(i, j)$ or $(j, i)$ is in $E(T)$. \cite{alon2016probabilistic} 
\end{definition}

The name of a tournament comes from the fact that it can be thought of as a sports tournament where each vertex represents a team and each team plays every other team exactly once. The edge $(i, j)$ represents a win for team $i$ over team $j$. A tournament $T$ has property $S_k$ if for every subset $K \subseteq V(T)$ of size $k$, there is a vertex $v \in V(T)$ such that $(v, s) \in E(T)$ for all $s \in K$ \cite{alon2016probabilistic}. That is, for every set of $k$ teams there is a team that beats all of them. For example, the tournament in Figure \ref{fig:tournament} has property $S_1$ since every team is beaten by another team. \par

A natural question to ask is: is there a tournament with property $S_k$ for every $k$? The answer is yes. We will prove this using the probabilistic method. First we define a probability space over the set of tournaments on $n$ vertices: \par 
A \textit{random} tournament on a set of $n$ vertices is a tournament $T$ such that for every pair of vertices $i, j \in V(T)$, the edge $(i, j)$ is in $E(T)$ with probability $\frac{1}{2}$ and the edge $(j, i)$ is in $E(T)$ with probability $\frac{1}{2}$, independently of all other edges. Thus, every tournament on $n$ vertices has the same probability, which means that this probability space is \textit{symmetric}. \par

\begin{figure}
    \centering
    \[\begin{tikzcd}
        & \bullet \\
        \\
        \bullet && \bullet
        \arrow[from=3-1, to=1-2]
        \arrow[from=1-2, to=3-3]
        \arrow[from=3-3, to=3-1]
    \end{tikzcd}\]
    \caption{A tournament on 3 vertices with property $S_1$.}
    \label{fig:tournament}
\end{figure}

The main idea is to show that there is a large $n$ as a function of $k$, such that the probability that a random tournament on $n$ vertices has property $S_k$ is greater than zero. This implies that there is at least one tournament with property $S_k$. \par


\begin{theorem}
    For every $k \in \N$, there is a tournament with property $S_k$. \cite{alon2016probabilistic}
\end{theorem}

\textbf{Proof. } Fix a subset $K \subseteq V(T)$ of size $k$. Consider the event $A_K$ that there is no vertex $v \in V(T)$ such that $(v, s) \in E(T)$ for all $s \in S$. For any vertex $v \in V(T) \setminus K$, the probability that $(v, s) \notin E(T)$ for all $s \in K$ is $2^{-k}$. Thus,
\[\Pr[A_K] = \left(1 - 2^{-k}\right)^{n - k}.\]
Now, if we consider all subsets $K \subseteq V(T)$ of size $k$, then the probability that $T$ does not have property $S_k$ is the probability that any of the events $A_K$ occurs. Since there are $\binom{n}{k}$ such subsets, by the union bound,
\[\Pr\left[\bigvee_{\substack{K \subseteq V(T) \\ |K| = k}} A_K\right] \leq \sum_{\substack{K \subseteq V(T) \\ |K| = k}}\Pr[A_K] = \binom{n}{k}\left(1 - 2^{-k}\right)^{n - k}.\]
We want to show that, for some $n$, this probability of this event is less than 1. Using Propositions \ref{ap:prop:upperbinom} and \ref{ap:prop:exp}, we have that
\begin{align}
    \Pr\left[\bigvee_{\substack{K \subseteq V(T) \\ |K| = k}} A_K\right] &\leq \binom{n}{k}\left(1 - 2^{-k}\right)^{n - k} \\
    &\leq \left(\frac{en}{k}\right)^k\left(e^{-2^{-k}}\right)^{n - k} = e^{k\log\left(\frac{n}{k}\right) - \frac{n - k}{2^k}}. \label{eq:tournament}
\end{align}
Then, \ref{eq:tournament} is less than one if 
\[k\log\left(\frac{n}{k}\right) - \frac{n - k}{2^k} = k\log n - k\log k + \frac{k}{2^k} - \frac{n}{2^k} < k\log n - \frac{n}{2^k} < 0,\]
which holds if and only if
\[\frac{n}{\log n} > k2^k.\]
Now, if $n = k^2 2^k\log 2$, then
\[\lim_{n \to \infty} \frac{n}{k2^k\log n} = \lim_{k \to \infty} \frac{k\log 2}{2\log k + k\log 2 + \log\log 2} = 1.\]
Hence, we conclude that there exists $n = (\log 2)k^2 2^k (1 + o(1))$, such that the probability that a random tournament on $n$ vertices does not have property $S_k$ is less than one. Therefore, the probability that there exists a tournament on $n$ vertices with property $S_k$ is greater than zero, which means that there exists at least one tournament with property $S_k$. \qed

We make two observations: 
\begin{enumerate}
    \item We used the \textit{union bound}. The union bound is a common technique in the probabilistic method. It states that for any events $A_1, ..., A_n$,
    \[\Pr[A_1 \cup ... \cup A_n] \leq \Pr[A_1] + ... + \Pr[A_n].\]
    We will extensively use this technique in this thesis. In a measure space, the union bound is the same property as \textit{subadditivity}. \par
    \item The proof is nonconstructive. It does not give us a way to find a tournament with property $S_k$. It only shows that there is at least one. This is a common feature of the probabilistic method. However, in this case, we have that for large enough $n$, the probability that a random tournament on $n$ vertices has property $S_k$ is close to one. This means that we can find a tournament with property $S_k$ by generating random tournaments until we find one with the desired property. If $n$ is large enough, this will not take too long. \par
\end{enumerate}
In this chapter, we will introduce some tools that are useful for applying the probabilistic method in discrete settings. We will also give some examples of the method in action. \par

\section{Linearity of Expectation}\label{sec:probmet:linearity}

Let $X$ be a discrete random variable, then the \textit{expected value} of $X$ is defined as 
\[E[X] = \sum_{x \in \text{Rg}(X)} x\Pr[X = x].\]
We show that the expected value is linear. \par
Let $X$ and $Y$ be discrete random variables. Then, the expected value of $X + Y$ is
\begin{align*}
    E[X + Y] &= \sum_{x \in \text{Rg}(X)} \sum_{y \in \text{Rg}(Y)} (x + y)\Pr[X = x \wedge Y = y] \\
    &= \sum_{x \in \text{Rg}(X)} \sum_{y \in \text{Rg}(Y)} x\Pr[X = x \wedge Y = y] + \sum_{x \in \text{Rg}(X)} \sum_{y \in \text{Rg}(Y)} y\Pr[X = x \wedge Y = y] \\
    &= \sum_{x \in \text{Rg}(X)} x\sum_{y \in \text{Rg}(Y)} \Pr[X = x \wedge Y = y] + \sum_{y \in \text{Rg}(Y)} y\sum_{x \in \text{Rg}(X)} \Pr[X = x \wedge Y = y] \\
    &= \sum_{x \in \text{Rg}(X)} x\Pr[X = x] + \sum_{y \in \text{Rg}(Y)} y\Pr[Y = y] \\
    &= E[X] + E[Y].
\end{align*}
Similarly, if $a \in \R$,
\begin{align*}
    E[aX] &= \sum_{x \in \text{Rg}(X)} ax\Pr[X = x] \\
    &= a\sum_{x \in \text{Rg}(X)} x\Pr[X = x] \\
    &= aE[X].
\end{align*}
This result is known as the \textit{linearity of expectation}. 
\begin{example}\label{ex:probmet:linearity}\cite{alon2016probabilistic}
    Let $\sigma$ be a random permutation of $\{1, \ldots, n\}$ chosen uniformly at random. Let $X_i$ be the indicator variable for the event that $\sigma(i) = i$. Then, $E[X_i] = \frac{1}{n}$ since there are $n$ possible values for $\sigma(i)$ and only one of them is $i$. Now, let $X = \sum_{i = 1}^n X_i$. Then, $X$ is the number of fixed points of $\sigma$. By the linearity of expectation,
    \[E[X] = E\left[\sum_{i = 1}^n X_i\right] = \sum_{i = 1}^n E[X_i] = \sum_{i = 1}^n \frac{1}{n} = 1.\tqed\]
\end{example}
Note that there is a point in the probability space $x \geq E[X]$ such that $\Pr[X = x] > 0$, and there is a point $x \leq E[X]$ such that $\Pr[X = x] > 0$. The following result by Szele (1943) is often considered as one of the first applications of the probabilistic method \cite{alon2016probabilistic}. \par
\begin{theorem}
    There is a tournament with $n$ players and at least $n!2^{-(n - 1)}$ Hamiltonian paths. \cite{alon2016probabilistic}
\end{theorem}
\textbf{Proof. } Let $X$ be the number of hamiltonian paths in a random tournament. Let $\sigma$ be a permutation, and let $X_\sigma$ be the indicator variable for the event that $\sigma$ is a hamiltonian path. That is, $\sigma$ is an ordering of the vertices such that $(\sigma(1), \sigma(2)), ..., (\sigma(n - 1), \sigma(n))$ are edges of the tournament. Then, $X = \sum_{\sigma} X_\sigma$. By the linearity of expectation,
\[E[X] = E\left[\sum_{\sigma} X_\sigma\right] = \sum_{\sigma} E[X_\sigma] = \sum_{\sigma} \frac{1}{2^{n - 1}} = n!2^{-(n - 1)}.\]
Therefore, there exists a tournament with at least $n!2^{-(n - 1)}$ hamiltonian paths. \qed

\section{Second Moment Method}\label{sec:probmet:secondmoment}

Just as we can use the linearity of expectation to prove results with the probabilistic method, we can also use the \textit{second moment method}, which relies on the \textit{variance} of a random variable. Let $X$ be a random variable with expected value $E[X]$. Then, the variance of $X$ is defined as
\[\text{Var}[X] = E[(X - E[X])^2].\]
By the linearity of expectation, 
\[E[(X - E[X])^2] = E[X] - 2E[XE[X]] + E[X]^2 = E[X^2] - E[X]^2.\]
The standard practice is to denote the expected value as $\mu$ and the variance as $\sigma^2$. The use of the following inequality is called the second moment method: \par

\begin{theorem}[Chebyschev's inequality]\label{thm:probmet:chebyschev}\cite{alon2016probabilistic}
    For $\lambda \geq 0$,
\end{theorem}
\[\Pr[|X - \mu| \geq \lambda \sigma] \leq \frac{1}{\lambda^2}.\]
\textbf{Proof. } 
\[\sigma^2 = \text{Var}[X] = E[(x - \mu)^2] \geq \lambda^2\sigma^2 \Pr[|X - \mu| \geq \lambda \sigma]. \qed\]

If $X = X_1 + ... + X_n$, then, by the linearity of expectation,
\[\text{Var}[X] = \sum_{i = 1}^{n}\text{Var}[X_i] + \sum_{i \neq j}\text{Cov}[X_i, X_j],\]
where 
\[\text{Cov}[X_i, X_j] = E[X_iX_j] - E[X_i]E[X_j].\] 
Furthermore, if, for each $i$, $X_i$ is an indicator variable of event $A_i$, that is, $X_i = 1$ if $A_i$ occurs and $X_i = 0$ otherwise, then
\[\text{Var}[X_i] = \Pr[A_i](1 - \Pr[A_i]) \leq E[X_i],\]
and we have that
\[\text{Var}[X] \leq E[X] + \sum_{i \neq j} \text{Cov}[X_i, X_j].\]

If $X$ only takes nonnegative integer values, we can bound $\Pr[X = 0]$ 

\section{Threshold Functions}\label{sec:probmet:threshold}

Let $n \in \N$ and $0 \leq p \leq 1$. The random graph $G(n, p)$ is a probability space over the set of graphs on $n$ labeled vertices determined by
\[\Pr[\{i, j\} \in G] = p\] 
with these events mutually independent \cite{alon2016probabilistic}. Given a graph theoretic property $A$, there is a probability that $G(n, p)$ satisfies $A$, which we write as $\Pr[G(n, p) \vDash A]$. 

\begin{definition}
    $r(n)$ is a threshold function for a graph theoretic property $A$ if 
    \begin{enumerate}
        \item When \(p(n) \in o(r(n)), \; \lim_{n \to \infty} \Pr[G(n, p(n)) \vDash A] = 0,\)
        \item When \(r(n) \in o(p(n)), \;  \lim_{n \to \infty} \Pr[G(n, p(n)) \vDash A] = 1,\) 
    \end{enumerate}
    or vice versa. \cite{alon2016probabilistic}
\end{definition}

We give an example of a threshold function which illustrates a common method for proving that a function is a threshold. \par

\subsection{Threshold function for having isolated vertices}

Let $G$ be a graph on $n$ labeled vertices. An isolated vertex of $G$ is a vertex which does not belong to any of the edges of $G$. Let $A$ be the property that $G$ contains an isolated vertex. We will prove that $\displaystyle{r(n) = \frac{\ln n}{n}}$ is a threshold for $A$. \par

For each vertex $i$ in $G$ define the variable 

\[X_i = 
\left\{
	\begin{array}{ll}
		1  & \mbox{if } i \text{ is an isolated vertex,} \\
		0 & \mbox{if } i \text{ is not an isolated vertex.}
	\end{array}
\right.
\]

Now, the probability that a vertex $i$ is isolated is $(1 - p)^{n - 1}$ since it is the probability that none of the other $n - 1$ vertices is connected to $i$. Let $X = \sum_{i = 1}^n X_i$, then the expected number of isolated vertices is
 \[E[X] = \sum_{i = 1}^{n} E[X_i] = \sum_{i = 1}^{n} \Pr[X_i] = n(1 - p)^{n - 1}.\]

Let $\displaystyle{p = k\frac{\ln n}{n}}$ for $k \in \R_{>0}$. Then
\begin{align*}
    \lim_{n \to \infty} E[X] &= \lim_{n \to \infty} n\left(1 - k\frac{\ln n}{n}\right)^{n - 1} \\
    &= ne^{-k\ln n} = n^{1 - k}.
\end{align*}

Therefore, $\lim_{n \to \infty} E[X] = 0$ if $k > 1$. Since \(E[X] \geq \Pr[X > 0],\) we conclude that \[\lim_{n \to \infty} \Pr[G(n, p) \vDash A] =  \lim_{n \to \infty} \Pr[X > 0] = 0.\] \par
Now, for $k < 1$, the fact that $\lim_{n \to \infty} E[X] = \infty$ is not enough to conclude that \(\lim_{n \to \infty} \Pr[G(n, p) \vDash A] = 1\). We have to use the second moment method. \par

\begin{theorem*}
    If $E[X] \to \infty$ and $\text{Var}[X] = o(E[X]^2)$, then $\lim_{n \to \infty} \Pr[X > 0] = 1$. \cite{alon2016probabilistic}
\end{theorem*}

\textbf{Proof. } We will prove that, in this case, $Var[X] = o(E[X]^2)$. First, 
\begin{align*}
    \sum_{i \neq j}E[X_iX_j] &= \sum_{i \neq j} \Pr[X_i = X_j = 1] \\
    &= n(n - 1)(1 - p)^{n -1}(1 - p)^{n - 2} \\ &= n(n - 1)(1 - p)^{2n - 3},
\end{align*}

for if $i$ is an isolated vertex, then there is no edge between $i$ and $j$ so we only have to account for the remaining $n - 2$ edges that contain $j$.  \par

Thus, since $\sum_{i = 1}^{n}E[X_i^2] =  \sum_{i = 1}^n E[X_i] = E[X]$ and $\lim_{n \to \infty} p = 0$,

\begin{align*}
    \lim_{n \to \infty} \frac{\text{Var}[X]}{E[X]^2} &= 
    \lim_{n \to \infty}\frac{E[X^2] - E[X]^2}{E[X]^2} = \lim_{n \to \infty} \frac{\sum_{i = 1}^n E[X_i^2] + \sum_{i \neq j}E[X_iX_j]}{E[X]^2} - 1 \\ &= 0 + \lim_{n \to \infty} \frac{ n(n - 1)(1 - p)^{2n - 3}}{n^2(1 - p)^{2n - 2}} - 1= \lim_{n \to \infty} \frac{1}{1 - p} - 1 = 0.\\
\end{align*} \par
We conclude that $\text{Var}[X] \in o(E[X]^2)$ and so, if $k < 1$, \[\lim_{n \to \infty} \Pr[G(n, p) \vDash A] = \lim_{n \to \infty} \Pr[X > 0] = 1.\] Therefore, $r(n) = \frac{\ln n}{n}$ is a threshold function for property $A$.
